{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "##INSTALLS"
      ],
      "metadata": {
        "id": "Rb4s3JaijzVo"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dW5NyxuV2hS2"
      },
      "outputs": [],
      "source": [
        "!pip install PyPDF2 langchain llama-index nltk faiss-cpu langchain-community\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RRl7AtEbmovV"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4o8rkZFt812a"
      },
      "outputs": [],
      "source": [
        "!pip install --upgrade openai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "FWuDRQRO2vwj"
      },
      "outputs": [],
      "source": [
        "%pip install -qU langchain-text-splitters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vKwT_94qPIP7"
      },
      "outputs": [],
      "source": [
        "!pip install chromadb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "wOiMoumNPmAk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2b0f5a5a-97d0-4961-d3cd-e16f3047fcc5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: lark in /usr/local/lib/python3.11/dist-packages (1.2.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install lark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MdEdldU5U8Gl"
      },
      "outputs": [],
      "source": [
        "!pip install unstructured"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FtvU0ZJzgQ6-"
      },
      "outputs": [],
      "source": [
        "!pip install unstructured[inference]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DpZG3o0ijvGu"
      },
      "outputs": [],
      "source": [
        "!apt-get install -y tesseract-ocr\n",
        "!pip install pytesseract"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hoXVgn-2g3MP"
      },
      "outputs": [],
      "source": [
        " pip install \"unstructured[local-inference]\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-fcAS_WRfUvo"
      },
      "outputs": [],
      "source": [
        "!pip install pdfminer.six"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hdsm5gtsfhO_"
      },
      "outputs": [],
      "source": [
        "!pip install pillow_heif"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z1W5hNCmjQWf"
      },
      "outputs": [],
      "source": [
        "!apt-get install poppler-utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qnibAuKSqKow"
      },
      "outputs": [],
      "source": [
        "!python -m nltk.downloader averaged_perceptron_tagger"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vV7Q8UzdMOE-"
      },
      "outputs": [],
      "source": [
        "!pip install llama-index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "REUEfwrZMJi0"
      },
      "outputs": [],
      "source": [
        "%pip install llama-index-vector-stores-faiss\n",
        "%pip install llama-index-vector-stores-pinecone\n",
        "%pip install llama-index-vector-stores-chroma"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4yA6rrTw3BLc"
      },
      "outputs": [],
      "source": [
        "pip install faiss-cpu  pinecone-client chromadb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uaPWOM9NMEhN"
      },
      "outputs": [],
      "source": [
        "%pip install llama-index-embeddings-huggingface\n",
        "%pip install llama-index-embeddings-langchain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RJt_dNg-wb_v"
      },
      "outputs": [],
      "source": [
        "!pip install sentence_transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "e7O-2bUYDico"
      },
      "outputs": [],
      "source": [
        "!pip install llama-index>=0.9.31 pinecone-client>=3.0.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "F8ivuaGvpfEt"
      },
      "outputs": [],
      "source": [
        "%pip install -qU ragchecker llama-index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GD2xiKv8ulSG"
      },
      "outputs": [],
      "source": [
        "!pip install ragchecker\n",
        "!python -m spacy download en_core_web_sm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "_0JhkYMpshid"
      },
      "outputs": [],
      "source": [
        "!pip install -qU scikit-learn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CqaEqLoqsl19"
      },
      "outputs": [],
      "source": [
        "!python -m spacy download en_core_web_sm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DVB4yXKYlvBq"
      },
      "outputs": [],
      "source": [
        "pip install scikit-learn-intelex\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zaLiQ9kiQy6l"
      },
      "outputs": [],
      "source": [
        "!pip install --upgrade scikit-learn\n",
        "!pip install --upgrade scikit-learn-intelex"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0omzAU1nTTKv"
      },
      "outputs": [],
      "source": [
        "!pip install refchecker==0.2.12"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tDBW6jocXPoW"
      },
      "outputs": [],
      "source": [
        "!pip install litellm==1.47.1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CfX9u2r5zMK2"
      },
      "source": [
        "##CHUNKING FUNCTIONS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "W87XuzW0nG-M"
      },
      "outputs": [],
      "source": [
        "############ SET YOUR API KEYS #################\n",
        "\n",
        "\n",
        "os.environ[\"AWS_ACCESS_KEY_ID\"] = \"set-your-key\"\n",
        "os.environ[\"AWS_SECRET_ACCESS_KEY\"] = \"set-your-key\"\n",
        "os.environ[\"AWS_REGION_NAME\"] = \"us-east-1\" # us-east-1, us-east-2, us-west-1, us-west-2\n",
        "os.environ[\"PINECONE_API_KEY\"] = \"set-your-key\"\n",
        "api_key = os.environ[\"PINECONE_API_KEY\"]\n",
        "openai_api_key = 'set-your-key'\n",
        "os.environ['OPENAI_API_KEY'] = openai_api_key"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "D_q3OYJCzYtc"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import nltk\n",
        "import openai\n",
        "from PyPDF2 import PdfReader\n",
        "from langchain_text_splitters import MarkdownHeaderTextSplitter, RecursiveCharacterTextSplitter, CharacterTextSplitter, MarkdownTextSplitter, LatexTextSplitter\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain.embeddings.openai import OpenAIEmbeddings\n",
        "from langchain.llms import OpenAI\n",
        "from langchain.retrievers.self_query.base import SelfQueryRetriever\n",
        "from langchain.chains.query_constructor.base import AttributeInfo\n",
        "from llama_index.core import SimpleDirectoryReader, StorageContext, VectorStoreIndex\n",
        "from llama_index.embeddings.openai import OpenAIEmbedding\n",
        "from langchain.schema import Document\n",
        "from langchain.vectorstores import Chroma"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "mgHY36v53Baw"
      },
      "outputs": [],
      "source": [
        "def read_pdf(file_path):\n",
        "    reader = PdfReader(file_path)\n",
        "    text = \"\"\n",
        "    for page_num in range(len(reader.pages)):\n",
        "        text += reader.pages[page_num].extract_text()\n",
        "    return text\n",
        "# Chunking strategies\n",
        "def fixed_length_chunking(text, chunk_size, chunk_overlap):\n",
        "    print(f\"Text length: {len(text)}\")\n",
        "    print(f\"Chunk size: {chunk_size}\")\n",
        "    print(f\"Chunk overlap: {chunk_overlap}\")\n",
        "    text_splitter = RecursiveCharacterTextSplitter(\n",
        "                     chunk_size = chunk_size,\n",
        "                     chunk_overlap = chunk_overlap)\n",
        "    chunks = text_splitter.create_documents([text])\n",
        "    print(f\"\\n Number of chunks created \\n: {len(chunks)}\")\n",
        "    print(\"------------------\")\n",
        "    return chunks\n",
        "\n",
        "def markdown_chunking(text, chunk_size, chunk_overlap):\n",
        "    markdown_splitter = MarkdownTextSplitter(\n",
        "        chunk_size=chunk_size,\n",
        "        chunk_overlap=chunk_overlap\n",
        "    )\n",
        "    chunks = markdown_splitter.create_documents([text])\n",
        "    return chunks\n",
        "\n",
        "def latex_chunking(text, chunk_size, chunk_overlap):\n",
        "    latex_splitter = LatexTextSplitter(\n",
        "        chunk_size=chunk_size,\n",
        "        chunk_overlap=chunk_overlap\n",
        "    )\n",
        "    chunks = latex_splitter.create_documents([text])\n",
        "    return chunks\n",
        "\n",
        "def create_embeddings(documents, persist_directory):\n",
        "    embeddings = OpenAIEmbeddings()\n",
        "    vector_store = Chroma.from_documents(documents=documents, embedding=embeddings, persist_directory=persist_directory)\n",
        "    return vector_store"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "_n9Gv5cWUr7c"
      },
      "outputs": [],
      "source": [
        "from unstructured.chunking.title import chunk_by_title\n",
        "from unstructured.partition.pdf import partition_pdf\n",
        "from langchain_community.vectorstores.utils import filter_complex_metadata\n",
        "\n",
        "def context_chunking(file_path):\n",
        "    elements = partition_pdf(file_path)\n",
        "\n",
        "    # print(f\"Partitioned Elements: {[el.text[:100] for el in elements]}\")\n",
        "    # print (\"----------------------------------\")\n",
        "\n",
        "    chunks = chunk_by_title(elements,combine_text_under_n_chars= 500, max_characters = 2048)\n",
        "\n",
        "    # print(f\"\\nChunked Elements: {[el.text[:100] for el in chunks]}\")\n",
        "    # print (\"----------------------------------\")\n",
        "\n",
        "    document_chunks = [Document(page_content=el.text, metadata={\"source\": el}) for el in chunks]\n",
        "    return filter_complex_metadata(document_chunks)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##HELPER FUNTIONS\n"
      ],
      "metadata": {
        "id": "23Ec8G_Lxvm0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "81VQu45kjkmR"
      },
      "outputs": [],
      "source": [
        "import csv\n",
        "import pandas as pd\n",
        "\n",
        "# Function to save RAGResults to CSV with chunking strategy, embedding, and vector store names\n",
        "def save_rag_results_to_csv(rag_results, chunking_strategy_name, embedding_strategy_name, vector_store_name, query_num, filename=\"rag_results.csv\"):\n",
        "    # Extracting metrics from RAGResults\n",
        "    overall_metrics = rag_results.metrics['overall_metrics']\n",
        "    retriever_metrics = rag_results.metrics['retriever_metrics']\n",
        "    generator_metrics = rag_results.metrics['generator_metrics']\n",
        "\n",
        "    # Creating a row of data with strategy names\n",
        "    row = {\n",
        "        'Query Number': query_num,\n",
        "        'Chunking Strategy': chunking_strategy_name,\n",
        "        'Embedding Strategy': embedding_strategy_name,\n",
        "        'Vector Store': vector_store_name,\n",
        "        'Precision': overall_metrics['precision'],\n",
        "        'Recall': overall_metrics['recall'],\n",
        "        'F1 Score': overall_metrics['f1'],\n",
        "        'Claim Recall': retriever_metrics['claim_recall'],\n",
        "        'Context Precision': retriever_metrics['context_precision'],\n",
        "        'Context Utilization': generator_metrics['context_utilization'],\n",
        "        'Noise Sensitivity (Relevant)': generator_metrics['noise_sensitivity_in_relevant'],\n",
        "        'Noise Sensitivity (Irrelevant)': generator_metrics['noise_sensitivity_in_irrelevant'],\n",
        "        'Hallucination': generator_metrics['hallucination'],\n",
        "        'Self Knowledge': generator_metrics['self_knowledge'],\n",
        "        'Faithfulness': generator_metrics['faithfulness']\n",
        "    }\n",
        "\n",
        "    # Append to CSV (use pandas for convenience)\n",
        "    try:\n",
        "        # Check if the file exists, to decide whether to write a header or not\n",
        "        write_header = not pd.io.common.file_exists(filename)\n",
        "\n",
        "        # Convert row to DataFrame and append to CSV\n",
        "        df = pd.DataFrame([row])\n",
        "        df.to_csv(filename, mode='a', index=False, header=write_header)  # 'a' mode for append\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error saving RAG results to CSV: {str(e)}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "@retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=4, max=10))\n",
        "async def wait_for_index_ready(pc, index_name, timeout=300):  # 5 minutes timeout\n",
        "    start_time = time.time()\n",
        "    while time.time() - start_time < timeout:\n",
        "        try:\n",
        "            # check if index exists and is ready\n",
        "            index = pc.Index(index_name)\n",
        "            stats = index.describe_index_stats()\n",
        "            if stats:  # index is ready\n",
        "                print(f\"Index {index_name} is ready\")\n",
        "                return True\n",
        "        except Exception as e:\n",
        "            print(f\"Index not ready yet: {e}\")\n",
        "\n",
        "        await asyncio.sleep(7)  # check every 7 seconds\n",
        "\n",
        "    raise TimeoutError(f\"Index {index_name} not ready after {timeout} seconds\")"
      ],
      "metadata": {
        "id": "lQktp5KGVl3w"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=4, max=10))\n",
        "async def setup_pinecone_index(pc, index_name):\n",
        "    try:\n",
        "        existing_indexes = pc.list_indexes()\n",
        "        if index_name not in [index.name for index in existing_indexes]:\n",
        "            print(f\"Creating new index: {index_name}\")\n",
        "            pc.create_index(\n",
        "                name=index_name,\n",
        "                dimension=1536,\n",
        "                metric=\"cosine\",\n",
        "                spec=ServerlessSpec(\n",
        "                    cloud=\"aws\",\n",
        "                    region=\"us-east-1\"\n",
        "                )\n",
        "            )\n",
        "            # Wait for index to be ready using polling\n",
        "            await wait_for_index_ready(pc, index_name)\n",
        "            print(f\"Index {index_name} created and ready\")\n",
        "        else:\n",
        "            pc.delete_index(index_name)\n",
        "            print(f\"Deleted old index: {index_name}\")\n",
        "            pc.create_index(\n",
        "                name=index_name,\n",
        "                dimension=1536,\n",
        "                metric=\"cosine\",\n",
        "                spec=ServerlessSpec(\n",
        "                    cloud=\"aws\",\n",
        "                    region=\"us-east-1\"\n",
        "                )\n",
        "            )\n",
        "            # Wait for index to be ready using polling\n",
        "            await wait_for_index_ready(pc, index_name)\n",
        "            print(f\"Index {index_name} created and ready\")\n",
        "\n",
        "        return pc.Index(index_name)\n",
        "    except Exception as e:\n",
        "        print(f\"Error in setup_pinecone_index: {e}\")\n",
        "        raise"
      ],
      "metadata": {
        "id": "fcqRP8N3hJ-Z"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##QUERIES AND GROUND TRUTH ANSWERS TO USE IN TESTING"
      ],
      "metadata": {
        "id": "BUxVUk8aybBN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Queries array\n",
        "queries = [\n",
        "    #e.g.\n",
        "    #1\n",
        "    #\"What is the primary function of the Cisco ME 4924-10GE Ethernet switch?\",\n",
        "]\n",
        "\n",
        "# Ground truth answers array\n",
        "ground_truth_answers = [\n",
        "    #e.g.\n",
        "    #\"The Cisco ME 4924-10GE Ethernet switch, also referred to as the switch, is a metro Ethernet switch that can be used as user facing provider edge aggregation equipment to connect to service provider customer routers, switches, or other devices. The switch can be deployed as a broadband aggregation switch, aggregating 1000BASE-X SFP Etherne traffic from other network devices to 10-Gigabit uplinks. It supports Layer 2, Layer 3, and Layer 4 switching services.\",\n",
        "\n",
        "]\n"
      ],
      "metadata": {
        "id": "JVOXwFwmJP3B"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WSkp1H0-DGMn"
      },
      "source": [
        "## MAIN CODE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "lGCGw9GoMUqy"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from llama_index.core import (\n",
        "    SimpleDirectoryReader,\n",
        "    load_index_from_storage,\n",
        "    VectorStoreIndex,\n",
        "    StorageContext,\n",
        ")\n",
        "from llama_index.embeddings.openai import OpenAIEmbedding\n",
        "import chromadb\n",
        "import faiss\n",
        "from llama_index.vector_stores.faiss import FaissVectorStore\n",
        "from pinecone import Pinecone, ServerlessSpec\n",
        "from llama_index.vector_stores.pinecone import PineconeVectorStore\n",
        "from llama_index.vector_stores.chroma import ChromaVectorStore\n",
        "from llama_index.core import StorageContext\n",
        "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
        "from IPython.display import Markdown, display\n",
        "from dotenv import load_dotenv\n",
        "from tenacity import retry, stop_after_attempt, wait_exponential\n",
        "import time\n",
        "import asyncio\n",
        "from typing import List\n",
        "import shutil\n",
        "from llama_index.core.storage.index_store import SimpleIndexStore\n",
        "from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n",
        "from llama_index.embeddings.langchain import LangchainEmbedding\n",
        "from llama_index.core import Document as LlamaDocument\n",
        "from langchain.schema import Document as LangchainDocument\n",
        "from llama_index.llms.openai import OpenAI\n",
        "from llama_index.core.evaluation import RelevancyEvaluator\n",
        "from ragchecker.integrations.llama_index import response_to_rag_results\n",
        "from ragchecker import RAGResults, RAGChecker\n",
        "from ragchecker.metrics import all_metrics\n",
        "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader\n",
        "import litellm\n",
        "from litellm import completion\n",
        "from sklearnex import patch_sklearn\n",
        "patch_sklearn()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nest_asyncio\n",
        "nest_asyncio.apply()"
      ],
      "metadata": {
        "id": "PSifItcca5ni"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file_path = \"your_file \" #eg. \"/content/me492410ge.pdf\"\n",
        "persist_directory = \"your_dir\" #eg. \"/content/vector_store_llamaIndex\""
      ],
      "metadata": {
        "id": "IsZIWBRrsWS5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pc = Pinecone(api_key=api_key)"
      ],
      "metadata": {
        "id": "Qj4eluZxjGq_"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "euZ-kf3pMtwv"
      },
      "outputs": [],
      "source": [
        "lc_embed_model = HuggingFaceEmbeddings(\n",
        "    model_name=\"sentence-transformers/all-mpnet-base-v2\" # 768 dimentions\n",
        ")\n",
        "mpnet_embeddings= LangchainEmbedding(lc_embed_model)\n",
        "\n",
        "openai_small_embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\") # 1536 dimentions\n",
        "openai_embeddings = OpenAIEmbeddings() #1536 dimentions (text-embedding-ada-002)\n",
        "\n",
        "\n",
        "async def test_chunking_strategies_with_llamaIndex_query(docs_path, chunk_size1, chunk_overlap1, chunk_size2, chunk_overlap2, persist_directory=persist_directory):\n",
        "\n",
        "    text = read_pdf(docs_path)\n",
        "\n",
        "    # Apply the different chunking strategies\n",
        "    strategies = {\n",
        "        \"fixed_length1\": fixed_length_chunking(text, chunk_size=chunk_size1, chunk_overlap=chunk_overlap1),\n",
        "        \"fixed_length2\": fixed_length_chunking(text, chunk_size=chunk_size2, chunk_overlap=chunk_overlap2),\n",
        "        \"context\": context_chunking(docs_path)\n",
        "    }\n",
        "\n",
        "    for chunking_strategy_name, chunks in strategies.items():\n",
        "      for embedding_strategy_name, embed_model in [(\"openai_small_embeddings\", openai_small_embeddings),\n",
        "                                                      (\"openai_embeddings\", openai_embeddings),\n",
        "                                                      (\"mpnet_embeddings\", mpnet_embeddings)]:\n",
        "\n",
        "\n",
        "        print(f\"___TESTING STRATEGY: {chunking_strategy_name} with {embedding_strategy_name}___\")\n",
        "\n",
        "        if embedding_strategy_name == \"mpnet_embeddings\":\n",
        "            dimension = 768\n",
        "            index_name = \"your_index_name\"\n",
        "        else:\n",
        "            dimension = 1536\n",
        "            index_name = \"your_index_name\"\n",
        "\n",
        "\n",
        "        #--------------Initialize vector stores---------------------\n",
        "\n",
        "        d = dimension\n",
        "\n",
        "        # ----Initialize FAISS vector store\n",
        "        faiss_index = faiss.IndexFlatL2(d)\n",
        "        faiss_vector_store = FaissVectorStore(faiss_index=faiss_index)\n",
        "        faiss_storage_context = StorageContext.from_defaults(vector_store=faiss_vector_store)\n",
        "\n",
        "\n",
        "        # ----Initialize Pinecone vector store\n",
        "        print(f\"-----Index name = {index_name}\")\n",
        "        pinecone_index = await setup_pinecone_index(pc, index_name)\n",
        "        pinecone_vector_store = PineconeVectorStore(pinecone_index=pinecone_index)\n",
        "        pinecone_storage_context = StorageContext.from_defaults(vector_store=pinecone_vector_store)\n",
        "        # Check if the index is being created correctly\n",
        "        index_description = pinecone_index.describe_index_stats()\n",
        "        time.sleep(8)\n",
        "        print(f\"----- Pinecone Index  status: {index_description}\")\n",
        "\n",
        "\n",
        "        # ----Initialize Chroma vector store\n",
        "        chroma_client = chromadb.EphemeralClient()\n",
        "        if \"quickstart-me492\" in chroma_client.list_collections():\n",
        "            chroma_client.delete_collection(\"quickstart-me492\")\n",
        "            chroma_client.create_collection(\"quickstart-me492\")\n",
        "        else:\n",
        "            chroma_client.create_collection(\"quickstart-me492\")\n",
        "        chroma_collection = chroma_client.get_collection(\"quickstart-me492\")\n",
        "        chroma_vector_store = ChromaVectorStore(chroma_collection=chroma_collection, embedding_function=embed_model)\n",
        "        chroma_storage_context = StorageContext.from_defaults(vector_store=chroma_vector_store)\n",
        "\n",
        "\n",
        "\n",
        "        for vector_store_name, storage_context in [(\"faiss\", faiss_storage_context), (\"pinecone\", pinecone_storage_context),\n",
        "                                                    (\"chroma\", chroma_storage_context)]:\n",
        "\n",
        "          print(f\"*Using vector store: {vector_store_name}\")\n",
        "\n",
        "          llama_index_docs = [LlamaDocument(text=c.page_content) for c in chunks]\n",
        "          i=0\n",
        "          try:\n",
        "              index = VectorStoreIndex.from_documents(llama_index_docs,\n",
        "                                                      embed_model=embed_model,\n",
        "                                                      storage_context=storage_context)\n",
        "\n",
        "              # index.storage_context.persist(persist_dir=persist_directory) if you want to persist in local directory\n",
        "\n",
        "          except Exception as e:\n",
        "             raise Exception(f\"Failed to index or persist the document: {str(e)}\")\n",
        "\n",
        "          # retriever = index.as_retriever(similarity_top_k=2)\n",
        "\n",
        "          query_engine = index.as_query_engine(similarity_top_k=3)\n",
        "\n",
        "          for query_num, (query, gt_answer) in enumerate(zip(queries, ground_truth_answers), start=1):\n",
        "\n",
        "              response = query_engine.query(query)\n",
        "\n",
        "              print(f\"\\nResponse from {query_num},   qeustion {query}   , from chunking strategy: {chunking_strategy_name} from embedding strategy:{embedding_strategy_name} and vector store {vector_store_name}\\n\")\n",
        "              print(response)\n",
        "\n",
        "              if response.response != 'Empty Response':\n",
        "                  rag_result = response_to_rag_results(\n",
        "                                query=query,\n",
        "                                gt_answer=gt_answer,\n",
        "                                response_object=response,\n",
        "                                )\n",
        "                  rag_result['query_id'] = str(query_num)\n",
        "                  print(rag_result)\n",
        "                  rag_results = RAGResults.from_dict({\"results\": [rag_result]})\n",
        "                  print(\"----Rag Results to evaluate----\")\n",
        "                  print(rag_results)\n",
        "                  # Initialize RAGChecker\n",
        "                  evaluator = RAGChecker(\n",
        "                                extractor_name=\"openai/gpt-4o-mini\",\n",
        "                                checker_name=\"openai/gpt-4o-mini\",\n",
        "                                batch_size_extractor=16,\n",
        "                                batch_size_checker=16\n",
        "                            )\n",
        "                  # Evaluate using RAGChecker\n",
        "                  evaluator.evaluate(rag_results, all_metrics)\n",
        "                  print(\"\\n----Output Results----\\n\")\n",
        "                  print(rag_results)\n",
        "                  save_rag_results_to_csv(rag_results, chunking_strategy_name, embedding_strategy_name, vector_store_name, query_num, filename=\"rag_res_me492.csv\")\n",
        "\n",
        "              else:\n",
        "                  print(f\"\\nSkipping evaluation from chunking:{chunking_strategy_name} embedding:{embedding_strategy_name} vector:{vector_store_name}due to empty response.\\n\")\n",
        "\n",
        "\n",
        "          print(\"\\n#########################################################\\n\")\n",
        "\n",
        "        print(\"Finished looping in vector stores\")\n",
        "\n",
        "\n",
        "\n",
        "# Example usage\n",
        "await test_chunking_strategies_with_llamaIndex_query(file_path, chunk_size1=1024, chunk_overlap1=128, chunk_size2=2048, chunk_overlap2=256, persist_directory=persist_directory)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8t8ia_ER6UNy"
      },
      "outputs": [],
      "source": [
        "from google.colab import files  # Only needed for Google Colab\n",
        "\n",
        "# Assuming the CSV is saved as 'rag_results.csv'\n",
        "files.download('rag_res_me492.csv')\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "Rb4s3JaijzVo",
        "CfX9u2r5zMK2",
        "BUxVUk8aybBN"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}